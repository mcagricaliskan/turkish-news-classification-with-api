{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehmet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mehmet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Category</th>\n",
       "      <th>Link</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Context</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ensonhaber</td>\n",
       "      <td>Automobile</td>\n",
       "      <td>https://www.ensonhaber.com/otomobil/turkiye-20...</td>\n",
       "      <td>Türkiye, 2020'de 9.5 milyar dolarlık binek oto...</td>\n",
       "      <td>Türkiye'den 2020'de 118 ülke ve özerk bölgeye ...</td>\n",
       "      <td>Uludağ Otomotiv Endüstrisi İhracatçıları Birli...</td>\n",
       "      <td>2021/01/20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ensonhaber</td>\n",
       "      <td>Living</td>\n",
       "      <td>https://www.ensonhaber.com/yasam/mpi-4-subat-2...</td>\n",
       "      <td>MPİ 3 Şubat 2022 Süper Loto sonuçları: Büyük i...</td>\n",
       "      <td>Milli Piyango İdaresi tarafından canlı çekilen...</td>\n",
       "      <td>3 Şubat 2022 Perşembe tarihli çekiliş sonuçlar...</td>\n",
       "      <td>2022/02/03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ensonhaber</td>\n",
       "      <td>Automobile</td>\n",
       "      <td>https://www.ensonhaber.com/otomobil/ilk-8-ayda...</td>\n",
       "      <td>İlk 8 ayda otomotiv üretimi yüzde 14 arttı</td>\n",
       "      <td>Ağustos sonu itibarıyla toplam otomotiv üretim...</td>\n",
       "      <td>Otomotiv Sanayii Derneği (OSD), ocak-ağustos d...</td>\n",
       "      <td>2021/09/14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ensonhaber</td>\n",
       "      <td>Health</td>\n",
       "      <td>https://www.ensonhaber.com/saglik/etten-daha-f...</td>\n",
       "      <td>Etten daha fazla protein içeren yer fıstığının...</td>\n",
       "      <td>İyi bir protein kaynağı olan ve aynı zamanda k...</td>\n",
       "      <td>Cips gibi tipik atıştırmalık yiyeceklerin çoğu...</td>\n",
       "      <td>2022/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ensonhaber</td>\n",
       "      <td>Living</td>\n",
       "      <td>https://www.ensonhaber.com/kadin/iletisim-kura...</td>\n",
       "      <td>İletişim kurarken güven vermenin en etkili 6 yolu</td>\n",
       "      <td>Sosyal ya da iş hayatında iletişim kurarken ka...</td>\n",
       "      <td>Kendine güven, becerilerinize, niteliklerinize...</td>\n",
       "      <td>2022/02/01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Source    Category                                               Link  \\\n",
       "0  Ensonhaber  Automobile  https://www.ensonhaber.com/otomobil/turkiye-20...   \n",
       "1  Ensonhaber      Living  https://www.ensonhaber.com/yasam/mpi-4-subat-2...   \n",
       "2  Ensonhaber  Automobile  https://www.ensonhaber.com/otomobil/ilk-8-ayda...   \n",
       "3  Ensonhaber      Health  https://www.ensonhaber.com/saglik/etten-daha-f...   \n",
       "4  Ensonhaber      Living  https://www.ensonhaber.com/kadin/iletisim-kura...   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Türkiye, 2020'de 9.5 milyar dolarlık binek oto...   \n",
       "1  MPİ 3 Şubat 2022 Süper Loto sonuçları: Büyük i...   \n",
       "2         İlk 8 ayda otomotiv üretimi yüzde 14 arttı   \n",
       "3  Etten daha fazla protein içeren yer fıstığının...   \n",
       "4  İletişim kurarken güven vermenin en etkili 6 yolu   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  Türkiye'den 2020'de 118 ülke ve özerk bölgeye ...   \n",
       "1  Milli Piyango İdaresi tarafından canlı çekilen...   \n",
       "2  Ağustos sonu itibarıyla toplam otomotiv üretim...   \n",
       "3  İyi bir protein kaynağı olan ve aynı zamanda k...   \n",
       "4  Sosyal ya da iş hayatında iletişim kurarken ka...   \n",
       "\n",
       "                                             Context        Date  \n",
       "0  Uludağ Otomotiv Endüstrisi İhracatçıları Birli...  2021/01/20  \n",
       "1  3 Şubat 2022 Perşembe tarihli çekiliş sonuçlar...  2022/02/03  \n",
       "2  Otomotiv Sanayii Derneği (OSD), ocak-ağustos d...  2021/09/14  \n",
       "3  Cips gibi tipik atıştırmalık yiyeceklerin çoğu...  2022/01/24  \n",
       "4  Kendine güven, becerilerinize, niteliklerinize...  2022/02/01  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_excel(\"Dataset/dataset_20_02_2022.xlsx\")\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "Automobile    5218\n",
       "Daily         5345\n",
       "Economy       5390\n",
       "Health        5383\n",
       "Living        5271\n",
       "Magazine      5329\n",
       "Sport         5356\n",
       "Technology    5371\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting Class Data Count\n",
    "dataset_df.groupby(\"Category\")[\"Category\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42663 data processing took 304.0950288772583 seconds\n"
     ]
    }
   ],
   "source": [
    "# processing data with NLTK\n",
    "\n",
    "# I used from snowballstemmer import TurkishStemmer for stemming but it takes too long not good performance!\n",
    "# I tried from TurkishStemmer import TurkishStemmer and it is slow too but not slower like snowballstemmer\n",
    "# TurkishStemmer library add 240 sec (4 min) to total time on 42663 docs (240/42663 = 0.005 sec actualy looks like good i will try it on API)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "\n",
    "stemmer = TurkishStemmer()\n",
    "\n",
    "processed_dataset_by_category = {\n",
    "\n",
    "}\n",
    "\n",
    "unique_words_by_class_with_count = {\n",
    "\n",
    "}\n",
    "\n",
    "def is_word_has_digit(word_string):\n",
    "    for char in word_string:\n",
    "        if char.isdigit():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "dataset_length = len(dataset_df)\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "turkish_stop_words = stopwords.words('turkish')\n",
    "# appanding more turkish stop words\n",
    "with open(\"Dataset/more_turkish_stop_words.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    new_stop_words = f.read().splitlines()\n",
    "    turkish_stop_words.extend(new_stop_words)\n",
    "\n",
    "# set will make all words unique and making if condition fast\n",
    "turkish_stop_words = set(turkish_stop_words)\n",
    "\n",
    "# create regex\n",
    "regex_tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "for index, row in dataset_df.iterrows():\n",
    "    unique_words_by_class_with_count.setdefault(row[\"Category\"], {})\n",
    "    processed_dataset_by_category.setdefault(row[\"Category\"], [])\n",
    "\n",
    "    tokenized_words = regex_tokenizer.tokenize(row[\"Context\"])\n",
    "    # making all words lower case\n",
    "    tokenized_words = [word.lower() for word in tokenized_words]\n",
    "    # remove turkish stop words\n",
    "    tokenized_words = [word for word in tokenized_words if word not in turkish_stop_words]\n",
    "    # remove digits \n",
    "    tokenized_words = [word for word in tokenized_words if not is_word_has_digit(word)]\n",
    "    # remove char lenth smaller than 2\n",
    "    tokenized_words = [word for word in tokenized_words if len(word) > 2]\n",
    "    # stemming words\n",
    "    tokenized_words = [stemmer.stem(word) for word in tokenized_words]\n",
    "    \n",
    "    # adding processed data to a list\n",
    "    processed_dataset_by_category[row[\"Category\"]].append(tokenized_words)\n",
    "    # setting and counting words\n",
    "    for word in tokenized_words:\n",
    "        unique_words_by_class_with_count[row[\"Category\"]].setdefault(word, 0)\n",
    "        unique_words_by_class_with_count[row[\"Category\"]][word] += 1\n",
    "\n",
    "print(f\"{dataset_length} data processing took {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automobile: [('sahip', 3798), ('elektrik', 4428), ('motor', 5910), ('model', 6154), ('satış', 6189), ('yen', 8117), ('yüz', 8376), ('yıl', 8475), ('otomobil', 10997), ('araç', 14040)]\n",
      "Living: [('mah', 5775), ('zaman', 5872), ('gün', 6622), ('yer', 6898), ('saat', 8095), ('bel', 10147), ('sokak', 14658), ('köy', 15136), ('mahalle', 15732), ('merkez', 18653)]\n",
      "Health: [('özellik', 6064), ('fazl', 6113), ('yardımç', 6271), ('etki', 6298), ('zaman', 6397), ('önem', 6567), ('gün', 6693), ('tedavi', 6895), ('hastalık', 12264), ('sağlık', 13989)]\n",
      "Daily: [('cumhurbaşkan', 3159), ('devam', 3197), ('son', 3276), ('ifade', 3310), ('karar', 3391), ('erdoğan', 3561), ('gün', 3777), ('yer', 4146), ('yıl', 5018), ('türki', 5547)]\n",
      "Sport: [('süper', 2662), ('kulüp', 2937), ('sezon', 3006), ('beşiktaş', 3220), ('son', 4029), ('galatasaray', 4587), ('fenerbahçe', 4628), ('lig', 6870), ('takım', 7025), ('maç', 11077)]\n",
      "Technology: [('oyun', 3145), ('özellik', 3360), ('apple', 3510), ('şirket', 3590), ('son', 3617), ('telefon', 4026), ('uzay', 4103), ('dünya', 4103), ('yen', 5332), ('yıl', 6904)]\n",
      "Economy: [('banka', 3830), ('son', 4307), ('do', 4611), ('sektör', 4704), ('artış', 4747), ('türki', 6359), ('fiyat', 6887), ('lira', 7969), ('yıl', 10842), ('yüz', 19404)]\n",
      "Magazine: [('şarkıç', 1904), ('paylaş', 1942), ('hesap', 2039), ('yaş', 2292), ('sosyal', 2712), ('medya', 2737), ('yıl', 2878), ('ünl', 3097), ('gün', 3177), ('oyunç', 3617)]\n"
     ]
    }
   ],
   "source": [
    "# Calculating most frequent words class by class\n",
    "\n",
    "max_values = 10\n",
    "for category, words_with_count in unique_words_by_class_with_count.items():\n",
    "    sorted_words_count = sorted(words_with_count.items(), key=lambda kv: kv[1])[-max_values:]\n",
    "    print(f\"{category}: {sorted_words_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setting dataset for training \n",
    "\n",
    "dataset_x = [\n",
    "\n",
    "]\n",
    "\n",
    "dataset_y = [\n",
    "\n",
    "]\n",
    "\n",
    "categories = list(processed_dataset_by_category.keys())\n",
    "default_label = [0 for i in range(len(categories))]\n",
    "\n",
    "\n",
    "for category, docs in processed_dataset_by_category.items():\n",
    "    for doc in docs:\n",
    "        dataset_x.append(\" \".join(doc))\n",
    "        label = default_label.copy()\n",
    "        label[categories.index(category)] = 1\n",
    "        dataset_y.append(label)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9116120a73e4c85693ba75cec13d1495ad1ecdb600f953e70464207228840cc"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
