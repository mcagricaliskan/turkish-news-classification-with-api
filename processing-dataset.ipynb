{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehmet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mehmet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Category</th>\n",
       "      <th>Link</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Context</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ensonhaber</td>\n",
       "      <td>Automobile</td>\n",
       "      <td>https://www.ensonhaber.com/otomobil/turkiye-20...</td>\n",
       "      <td>Türkiye, 2020'de 9.5 milyar dolarlık binek oto...</td>\n",
       "      <td>Türkiye'den 2020'de 118 ülke ve özerk bölgeye ...</td>\n",
       "      <td>Uludağ Otomotiv Endüstrisi İhracatçıları Birli...</td>\n",
       "      <td>2021/01/20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ensonhaber</td>\n",
       "      <td>Living</td>\n",
       "      <td>https://www.ensonhaber.com/yasam/mpi-4-subat-2...</td>\n",
       "      <td>MPİ 3 Şubat 2022 Süper Loto sonuçları: Büyük i...</td>\n",
       "      <td>Milli Piyango İdaresi tarafından canlı çekilen...</td>\n",
       "      <td>3 Şubat 2022 Perşembe tarihli çekiliş sonuçlar...</td>\n",
       "      <td>2022/02/03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ensonhaber</td>\n",
       "      <td>Automobile</td>\n",
       "      <td>https://www.ensonhaber.com/otomobil/ilk-8-ayda...</td>\n",
       "      <td>İlk 8 ayda otomotiv üretimi yüzde 14 arttı</td>\n",
       "      <td>Ağustos sonu itibarıyla toplam otomotiv üretim...</td>\n",
       "      <td>Otomotiv Sanayii Derneği (OSD), ocak-ağustos d...</td>\n",
       "      <td>2021/09/14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ensonhaber</td>\n",
       "      <td>Health</td>\n",
       "      <td>https://www.ensonhaber.com/saglik/etten-daha-f...</td>\n",
       "      <td>Etten daha fazla protein içeren yer fıstığının...</td>\n",
       "      <td>İyi bir protein kaynağı olan ve aynı zamanda k...</td>\n",
       "      <td>Cips gibi tipik atıştırmalık yiyeceklerin çoğu...</td>\n",
       "      <td>2022/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ensonhaber</td>\n",
       "      <td>Living</td>\n",
       "      <td>https://www.ensonhaber.com/kadin/iletisim-kura...</td>\n",
       "      <td>İletişim kurarken güven vermenin en etkili 6 yolu</td>\n",
       "      <td>Sosyal ya da iş hayatında iletişim kurarken ka...</td>\n",
       "      <td>Kendine güven, becerilerinize, niteliklerinize...</td>\n",
       "      <td>2022/02/01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Source    Category                                               Link  \\\n",
       "0  Ensonhaber  Automobile  https://www.ensonhaber.com/otomobil/turkiye-20...   \n",
       "1  Ensonhaber      Living  https://www.ensonhaber.com/yasam/mpi-4-subat-2...   \n",
       "2  Ensonhaber  Automobile  https://www.ensonhaber.com/otomobil/ilk-8-ayda...   \n",
       "3  Ensonhaber      Health  https://www.ensonhaber.com/saglik/etten-daha-f...   \n",
       "4  Ensonhaber      Living  https://www.ensonhaber.com/kadin/iletisim-kura...   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Türkiye, 2020'de 9.5 milyar dolarlık binek oto...   \n",
       "1  MPİ 3 Şubat 2022 Süper Loto sonuçları: Büyük i...   \n",
       "2         İlk 8 ayda otomotiv üretimi yüzde 14 arttı   \n",
       "3  Etten daha fazla protein içeren yer fıstığının...   \n",
       "4  İletişim kurarken güven vermenin en etkili 6 yolu   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  Türkiye'den 2020'de 118 ülke ve özerk bölgeye ...   \n",
       "1  Milli Piyango İdaresi tarafından canlı çekilen...   \n",
       "2  Ağustos sonu itibarıyla toplam otomotiv üretim...   \n",
       "3  İyi bir protein kaynağı olan ve aynı zamanda k...   \n",
       "4  Sosyal ya da iş hayatında iletişim kurarken ka...   \n",
       "\n",
       "                                             Context        Date  \n",
       "0  Uludağ Otomotiv Endüstrisi İhracatçıları Birli...  2021/01/20  \n",
       "1  3 Şubat 2022 Perşembe tarihli çekiliş sonuçlar...  2022/02/03  \n",
       "2  Otomotiv Sanayii Derneği (OSD), ocak-ağustos d...  2021/09/14  \n",
       "3  Cips gibi tipik atıştırmalık yiyeceklerin çoğu...  2022/01/24  \n",
       "4  Kendine güven, becerilerinize, niteliklerinize...  2022/02/01  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_excel(\"Dataset/raw_dataset_20_02_2022.xlsx\")\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "Automobile    5218\n",
       "Daily         5345\n",
       "Economy       5390\n",
       "Health        5383\n",
       "Living        5271\n",
       "Magazine      5329\n",
       "Sport         5356\n",
       "Technology    5371\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Data count per class\n",
    "dataset_df.groupby(\"Category\")[\"Category\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42663 data processing took 379.8416533470154 seconds\n"
     ]
    }
   ],
   "source": [
    "# processing data with NLTK\n",
    "\n",
    "# I tried \"from snowballstemmer import TurkishStemmer\" for stemming but it takes too long\n",
    "# snowballTurkishStemmer took 0.018998384475708008 seconds! for one doc\n",
    "# I also tried \"from TurkishStemmer import TurkishStemmer\". It is slow too but it is faster than snowballstemmer\n",
    "# TurkishStemmer took 0.005014657974243164 seconds! for one doc\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "\n",
    "\n",
    "stemmer = TurkishStemmer()\n",
    "\n",
    "processed_dataset_by_category = {\n",
    "\n",
    "}\n",
    "\n",
    "unique_words_by_class_with_count = {\n",
    "\n",
    "}\n",
    "\n",
    "def is_word_has_digit(word_string):\n",
    "    for char in word_string:\n",
    "        if char.isdigit():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "dataset_length = len(dataset_df)\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "turkish_stop_words = stopwords.words('turkish')\n",
    "# appanding more turkish stop words\n",
    "with open(\"Models/more_turkish_stop_words.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    new_stop_words = f.read().splitlines()\n",
    "    turkish_stop_words.extend(new_stop_words)\n",
    "\n",
    "# using set will make all words unique and it will make \"if\" condition faster (because it is use hashtable)\n",
    "turkish_stop_words = set(turkish_stop_words)\n",
    "\n",
    "# create regex\n",
    "regex_tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "for index, row in dataset_df.iterrows():\n",
    "    unique_words_by_class_with_count.setdefault(row[\"Category\"], {})\n",
    "    processed_dataset_by_category.setdefault(row[\"Category\"], [])\n",
    "\n",
    "    tokenized_words = regex_tokenizer.tokenize(row[\"Context\"])\n",
    "    # make all words lower case\n",
    "    tokenized_words = [word.lower() for word in tokenized_words]\n",
    "    # remove turkish stop words\n",
    "    tokenized_words = [word for word in tokenized_words if word not in turkish_stop_words]\n",
    "    # remove digits \n",
    "    tokenized_words = [word for word in tokenized_words if not is_word_has_digit(word)]\n",
    "    # remove char lenth smaller than 2\n",
    "    tokenized_words = [word for word in tokenized_words if len(word) > 2]\n",
    "    # stem words\n",
    "    tokenized_words = [stemmer.stem(word) for word in tokenized_words]\n",
    "\n",
    "    \n",
    "    # adding processed data to a list\n",
    "    processed_dataset_by_category[row[\"Category\"]].append(tokenized_words)\n",
    "    # set and count words\n",
    "    for word in tokenized_words:\n",
    "        unique_words_by_class_with_count[row[\"Category\"]].setdefault(word, 0)\n",
    "        unique_words_by_class_with_count[row[\"Category\"]][word] += 1\n",
    "\n",
    "print(f\"{dataset_length} data processing took {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automobile: [('sahip', 3798), ('elektrik', 4428), ('motor', 5910), ('model', 6154), ('satış', 6189), ('yen', 8117), ('yüz', 8376), ('yıl', 8475), ('otomobil', 10997), ('araç', 14040)]\n",
      "Living: [('mah', 5775), ('zaman', 5872), ('gün', 6622), ('yer', 6898), ('saat', 8095), ('bel', 10147), ('sokak', 14658), ('köy', 15136), ('mahalle', 15732), ('merkez', 18653)]\n",
      "Health: [('özellik', 6064), ('fazl', 6113), ('yardımç', 6271), ('etki', 6298), ('zaman', 6397), ('önem', 6567), ('gün', 6693), ('tedavi', 6895), ('hastalık', 12264), ('sağlık', 13989)]\n",
      "Daily: [('cumhurbaşkan', 3159), ('devam', 3197), ('son', 3276), ('ifade', 3310), ('karar', 3391), ('erdoğan', 3561), ('gün', 3777), ('yer', 4146), ('yıl', 5018), ('türki', 5547)]\n",
      "Sport: [('süper', 2662), ('kulüp', 2937), ('sezon', 3006), ('beşiktaş', 3220), ('son', 4029), ('galatasaray', 4587), ('fenerbahçe', 4628), ('lig', 6870), ('takım', 7025), ('maç', 11077)]\n",
      "Technology: [('oyun', 3145), ('özellik', 3360), ('apple', 3510), ('şirket', 3590), ('son', 3617), ('telefon', 4026), ('uzay', 4103), ('dünya', 4103), ('yen', 5332), ('yıl', 6904)]\n",
      "Economy: [('banka', 3830), ('son', 4307), ('do', 4611), ('sektör', 4704), ('artış', 4747), ('türki', 6359), ('fiyat', 6887), ('lira', 7969), ('yıl', 10842), ('yüz', 19404)]\n",
      "Magazine: [('şarkıç', 1904), ('paylaş', 1942), ('hesap', 2039), ('yaş', 2292), ('sosyal', 2712), ('medya', 2737), ('yıl', 2878), ('ünl', 3097), ('gün', 3177), ('oyunç', 3617)]\n",
      "Total Unique Word Count: 342446\n"
     ]
    }
   ],
   "source": [
    "# Calculate most frequent words per class\n",
    "\n",
    "total_unique_word_count = 0\n",
    "max_values = 10\n",
    "for category, words_with_count in unique_words_by_class_with_count.items():\n",
    "    total_unique_word_count += len(words_with_count)\n",
    "    sorted_words_count = sorted(words_with_count.items(), key=lambda kv: kv[1])[-max_values:]\n",
    "    print(f\"{category}: {sorted_words_count}\")\n",
    "\n",
    "print(f\"Total Unique Word Count: {total_unique_word_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dataset for training \n",
    "\n",
    "dataset_x = []\n",
    "dataset_y = []\n",
    "\n",
    "categories = list(processed_dataset_by_category.keys())\n",
    "default_label = [0 for i in range(len(categories))]\n",
    "\n",
    "for category, docs in processed_dataset_by_category.items():\n",
    "    for doc in docs:\n",
    "        dataset_x.append(\" \".join(doc))\n",
    "        label = default_label.copy()\n",
    "        label[categories.index(category)] = 1\n",
    "        dataset_y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tf_idf_dataset = {\n",
    "    \"Dataset\": []\n",
    "}\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True,\n",
    "                                ngram_range=(1, 2),\n",
    "                                max_features=5000)\n",
    "\n",
    "vectors = vectorizer.fit_transform(dataset_x)\n",
    "dense = vectors.todense()\n",
    "\n",
    "# saving vectorizer for using in api\n",
    "# vectorizer fitted with using processed dataset\n",
    "# for getting tf-idf values for a unseen data (which is not include dataset) we need to use this vectorizer \n",
    "pickle.dump(vectorizer, open(f\"Models/tfidf_vectorizer.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ikna',\n",
       " 'ulusal',\n",
       " 'görmek',\n",
       " 'harcama',\n",
       " 'yaklaş',\n",
       " 'gerçekleşen',\n",
       " 'etkileyen',\n",
       " 'lif',\n",
       " 'hayati',\n",
       " 'il',\n",
       " 'dans',\n",
       " 'özel',\n",
       " 'pilot',\n",
       " 'mağlubiyet',\n",
       " 'zafer']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save processed tf-idf data\n",
    "import random\n",
    "# we are using tolist() because we want to save as json\n",
    "tf_idf_dataset[\"FeatureNames\"] = vectorizer.get_feature_names_out().tolist()\n",
    "\n",
    "# get 15 extracted feature from tf-idf vectorizer\n",
    "random.sample(tf_idf_dataset[\"FeatureNames\"], 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_dataset[\"ClassNames\"] = categories\n",
    "\n",
    "# also i will save class names for api\n",
    "with open(\"Models/class_names.txt\", \"a\") as f:\n",
    "    for category in categories:\n",
    "        f.write(f\"{category}\\n\")\n",
    "\n",
    "for data_x, data_y in zip(dense, dataset_y):\n",
    "    tf_idf_dataset[\"Dataset\"].append({\"x\": data_x.tolist()[0], \"y\": data_y}) \n",
    "   \n",
    "# save dataset for any crash  \n",
    "# i can't upload this file to github because it is 1GB so i will upload this to drive and i will share link in README of Dataset folder or you can run this .ipynb to extract it\n",
    "json.dump(tf_idf_dataset, open(\"Dataset/tf-idf-dataset.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9116120a73e4c85693ba75cec13d1495ad1ecdb600f953e70464207228840cc"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
